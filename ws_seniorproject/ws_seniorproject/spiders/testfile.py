import scrapy
from ws_seniorproject.items import WsSeniorprojectItem

class TestfileSpider(scrapy.Spider):
    
    # start section 1:
    # user search input
    # this allows users to input the items they would like to search for (i.e shoes, pants, tv's).
    # Amazon's url for a search looks like this: https://www.amazon.com/s?k= 
    # if we want to search for shoes, we add 'shoes' at the end of this usrl (https://www.amazon.com/s?k=shoes)
    # if the search is more than one word (i.e 'nike shoes', 'khaki pants'), the url requires a '+' between words
    # example: user input = nike shoes 
    # url = https://www.amazon.com/s?k=nike+shoes
    # the following code will generate such url and input it into 'start_urls'

    name = 'testfile'
    allowed_domains = ['amazon.com']
    search_terms = input("Enter search term: ")
    print("Search term: " + search_terms)
    
    search_terms_list = search_terms.split()
    
    if len(search_terms_list) == 1:
        url_to_crawl = 'https://www.amazon.com/s?k=' + search_terms
    else:
        url_to_crawl = 'https://www.amazon.com/s?k=' + search_terms_list[0]
        for remaining_terms in search_terms_list[1:]:
            url_to_crawl = url_to_crawl + "+" + remaining_terms

    
    
    # start_urls is the list of urls scrapy automatically looks for to crawl when the program is run. We are only
    # feeding it one url: the one generated by our code above. 

    start_urls = [url_to_crawl]

    # end section 1

    # start section 2
    # the follwowing function is the default parsing function called by scrapy to crawl the urls in 'start_urls'
    # the function tells scrapy what to scrape from the url. For this project, multiple urls have to be crawled:

    def parse(self, response):
        
        # the follwing line of code will look for all product urls after we search for a product. Amazon lists about 50-60
        # products for every search. However, as of now we are asking scrapy to extract the first 9 product urls [0:9]
        # and store them in a list named 'urls'

        # bug that was fixed: the code generated a url and an unwanted duplicate of the url. This was fixed by combining
        # css and xpath selectors to look for an h2 tag with the classes a-size-mini, a-spacing-none, and s-line-clamp-2
        # the xpath selector would then look for the href value and return the url. The duplicate urls came from not including
        # the classes in the h2 tag above the href value.

        urls = response.css('h2.a-size-mini.a-spacing-none.a-color-base.s-line-clamp-2 a').xpath('@href')[0:9].extract()
        
        # each url from the list 'urls' must individually be crawled. To allow for this, we run a for-loop that will
        # send a request to crawl a new url. For each url crawl request, we call the function 'parse_link'. 
        # parse_link will tell scrapy what we want to scrape from each url containing our individual products.  
        
        for product in urls:
            product_url = "https://www.amazon.com" + product
            request = scrapy.Request(product_url, callback=self.parse_link)
            yield request    

    # end section 2

    # start section 3
    # the following function 'parse_link' is called for every product. It currently scrapes the following fields:
    # 
    # product name
    # product price
    # product rating (out of 5 stars)
    # number of ratings
    # percent of ratings that are 5 stars
    # percent of ratings that are 4 stars
    # percent of ratings that are 3 stars
    # percent of ratings that are 2 stars
    # percent of ratings that are 1 stars

    # bugs that were fixed:
    # product name returned multiple spaces and '\n' fields along with the text for the product name. This was fixed by
    # simply calling the .strip() function on the extracted result. To allow the .strip() function to be added, 'title'
    # had to return a string instead of a list, which was made possible by adding '[0]' to the code, returning the first
    # value in the list as a string. This was convenient as all the other values following product name 
    # also returned strings.

    # percents of 1 - 5 stars was not returning the right values. This was fixed by combining css and xpath selectors
    # to allow the 'aria-label' to be selected. Additionally, the 6th - 10th values in the list returned the values we 
    # needed, which is why these are called in order: 5, 6, 7, 8, and 9.

    def parse_link(self, response):
        items = WsSeniorprojectItem()
        title = response.css('span.a-size-large#productTitle::text')[0].extract().strip(' \n')
        price = response.css('span.a-size-medium.a-color-price.priceBlockBuyingPriceString#priceblock_ourprice::text')[0].extract()
        rating = response.css('span.a-icon-alt::text')[0].extract()
        rating_count = response.css('span.a-size-base#acrCustomerReviewText::text')[0].extract()
        percent_5_star = response.css('div.a-meter').xpath('@aria-label')[5].extract()
        percent_4_star = response.css('div.a-meter').xpath('@aria-label')[6].extract()
        percent_3_star = response.css('div.a-meter').xpath('@aria-label')[7].extract()
        percent_2_star = response.css('div.a-meter').xpath('@aria-label')[8].extract()
        percent_1_star = response.css('div.a-meter').xpath('@aria-label')[9].extract()
        items['product_name'] = title
        items['product_sale_price'] = price
        items['product_rating'] = rating
        items['rating_count'] = rating_count
        items['percent_5_stars'] = percent_5_star
        items['percent_4_stars'] = percent_4_star
        items['percent_3_stars'] = percent_3_star
        items['percent_2_stars'] = percent_2_star
        items['percent_1_stars'] = percent_1_star
        yield items

    # end section 3
